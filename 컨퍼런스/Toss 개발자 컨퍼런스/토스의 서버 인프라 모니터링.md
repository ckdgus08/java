### Toss 서버 인프라 모니터링

#### 모니터링 인프라 구성

초기 : influxDB + telegraf 사용 쿠버네티스로 마이그레이션한 이후 : prometheus기반의 서비스들을 사용

보통은 오류로그를 보고 문제를 판단하게 되지만, 트랜젝션이 길거나 로직이 복잡하면 오류파악이 쉽지않다. 간단한 오류의 경우에 대부분 배포단계에서 해결이된다.

#### Metick

- Application Layer Metric
    - toss는 spring framework기반의 서비스가 대부분이다. ( jvm, tomcat, jpa )
    - node, python, golang 등의 서비스도 있다.
    - 로깅과 상관관계가 가장 높은 레이어이다.

- Network Layer Metric
    - 서비스와 서비스간의 통신에 관련된

- OS Layer Metric
    - OS 리소스 관련 지표
    - 오류와 가장 상관관계가 낮음.
    - cpu 사용량이 높다하여 어플리케이션 작동이 정상적인지 아닌지 선뜻 판단하기는 어렵다.
    - 개인적인 인사이트에 의존도가 높았음.

사용자가 어플리케이션에 접근하는데 여러 네트워크 홉(단계)를 거치게 되는데, 서비스 단계에 도달하기전에 문제가 발생하면 서비스까지 트래픽요청이 오지 않는다. 이러한 과정들이 가시화되지 않으면 단순히 트래픽이
떨어졌다고 판단 할 수 있는데, 이를 해결하기 위해 가시성 확보가 필요하고 auto failover 구성이 필요하다. 어떤 문제가 발생했는지 판단하기 위해서는 클라이언트 로그를 확인해보는 수 밖에 없다. 외부에서
여러 블랙박스테스트 모니터링 진행하고 있다.

AWS route 53을 통해서 블랙박스 모니터링 진행 중

1. 정상일 경우, 양쪽 IDC를 번걸아가며 리졸빙
2. 정상이 아닐경우, application까지 도달하지 못하여 healthcheck 실패

컨테이너 오케스트레이션 (쿠버네티스) 등 을 사용하면, cpu나 메모리는 독립적으로 사용 할 수 있지만 네트워크나 디스크의 경우에 서로의 성능에 영향을 줄 수 있기 때문에 하나의 대시보드에 모니터링을 진행 중.

#### 프로메테우스 단점 및 해결과정

- 스스로 어플리케이션의 메트릭을 스크랩하는 방식이라, 프로메테우스가 죽으면 당시의 메트릭은 누락이됨.
- 메모리 이슈가 많다. (사용되는 메모리는 수립되는 메트릭의양과 tsdb 저장량에 비례한다.) oom이 발생하면 프로메테우스가 종료되고 당시 메트릭이 누락됨.
- 메모리 이슈 해결 방법 : 수집하는 메트릭의 수를 최대한 줄이고 (querycount, 어그리게이션 축소 등) 저장되는 tsdb 양을 줄이는 것을 시도함.
- 처음에는 효과를 보았지만, 규모가 커질수록 한계에 다다름(최소한 가시성이 있기 때문에)
- 프로메테우스를 스케일아웃하여 (hashmode를 사용하여 샤딩) 해결
- 각 메트릭마다 db를 분리했을때는, 특정 사용량이 많은 메트릭을 샤딩하게 되야하기에 어플리케이션에 적용해야하기 때문에 운영이 쉽지않았다.

#### 종합

- 네트워크 제어권을 강화하여 네트워크 문제를 판단할 수 있는 메트릭 생성
- 발생하는 이유와 메트릭 사이에 상관관계 높임
- 모니터링 인프라도 스케일 아웃 가능하도록 구현
- 계속해서 높아지는 요구사항에 대응